1.

Work in this thesis is based on assumption that we already have good sketches. First this thesis must express
motivation what are sketches, why are sketchces important and what is good sketch according to DISA research.

Then we need to explain that in order for sketches to serve their purpose it's needed to find a good indexing approach
to speed up in-memory lookups. We should also mention that we only consider Hamming distance for reasons of HW implementation.

One promising approach for such task is Multi-Index hashing. We need to evaluate this approach and measure some metrics
against sequential scan. Metrics could be numbers of hamming distance computation and time of search in different scenarios.
Experiments should also evaluate performance of method based on selected substring size m and range r.

MIH is based on dividng Sketch in m substrings. For range query r query string h and candidate g it must hold that
they differ in at least substring i in at most floor(r/m) bits. Special case which is the most interesting is that when
r < m there must be at least one identical substring to satisfy condition of r-range distance.

Let p be length of Sketch in extreme case we choose m=p thus length of substring will be equal 1. Now r is most likely
be less than m but we might need to examine almost all buckets because except from those who differ in all bits (inverted str)
they will satisfy the condition.

Interesting idea might be to restrict range r to be less than m and store all known sketch addresses in some hash table
directing where to find them.

2.

In this thesis we should take advantage of the fact, that we know the whole database we're searching on in advance which
isn't the case in the MIH article. We should restrict r < m because then we can focus on exact matches of substrings only.
Assumption is that in k-NN queries usually most similar objects are those very close to query object.
Original article reduced search space so we don't visit empty buckets. We can avoid this entirely because we know
which buckets are not empty.

There will be 2 levels of index:

Single lower level hash-map will contain key: value items in format: {sketch: set(object id's with this sketch)}.
Then m upper level hash-maps will contain key: value items in format: {sketch_substring: set(sketches with corresponding substring)}

First we join all sets of sketch candidates from upper level hash maps. Then we retrieve all objects stored at these
sketchces addresses in lower level hash-map.

Disadvantage of this approach is, that each sketch will appear m-times - once in every upper level hash-map.

Question is how to choose m. With larger m more candidates will be returned as candidate set. With smaller m
range might be too restricted and not enough objects might be this close to query. It might be useful to
create multiple Multi-indexes. Those with small m and large r restriction will be very fast and there's possibility
to return enough objects for k-NN query. If not enough objects were returned another Multi-Index might be used
to search for further candidates and so on.

3.

Levels lower and upper level indexes might be totally separated. Ideas for levels of index:

Low-level: Sketch Object Storage.
Init:
SOS will receive iterator over sketchces and corresponding object id's and store them in sketch: set(object id's) format.
K-NN Query:
For querying it will receive set of candidate Sketchces, query Sketch and k. It will sort Sketches in set by similarity to query Sketch
and collect Object IDs for these Sketchces until it finds k nearest objects or less if not enough objects were in bucket.

Mid-level: Restricted Hash Multi-Index
Init:
RHMI will receive m, and iterator over all sketches. It will split each sketch into m splits and store them in format:
[sketch_split_id]: sketch_split: set(sketchces with same corresponding substring)
k-NN query:
For querying it will receive k, and set of sketchces to exclude. It will split query string to m substrings, for each
substring collect all sketches stored under this substring in hash-map which are not among sketchces to exclude. It will
query SOS with union of these sketches candidates and k and will receive set of candidate objects. It will return
candidate sketches, remaining k and collected candidate objects.

Upper-level: Restricted Hash Multi-Index Aggregator
Init:
Aggregator will receive iterator over all sketchces and sorted list M containing values of m for initalizing Hash Multi-Indexes.
For each value in M it will initialize one Multi-Index. First is the one that searches fast for closest objects last is that
which might be slower but cover more objects.
k-NN query:
Util result candidate set contains k object ID's or time ran up it will send queries to RHMIs in order. It will collect set
of result object IDs. For each next RHMI it will make union of Sketchces previous RHMIs returned for exclusion,
 so their buckets in SOS are not examined multiple times. If count of objects is equal or greater than k it
will return list of candidates.

notes:
- k in KNN query is requested size of candidates for further sequential scan.
- when collecting result object ID's in order of distance of sketchces from query sketch real distance of objects
might be computed to return them sorted. That might be implemented in later version. Maybe it's possible to yield
sets of objects for each sketch distance separately and compute those expensive distances for each sketch distance during the process.
- one of advantages of this approach is that whole mid-level might be deployed on separate cluster nodes to parallelize whole process.
Key,Value storage such as Redis could be used for this task.

4.

*implement all result retieval procedures by yielding to enable batch processing of results

5.

Note that this Index will be used on "perfect" sketches, which means that bits are close to uncorrelated, so distribution
of objects under different sketchces might be uniformly distributed and probably all buckets will contain some objects because
bits with correlation == 1 are useless. Anyway even now approach should be appliable. If m gets bigger, than only candidates
within close range are examined thus search should be pretty fast. In worst case what happens is that we won't be able to find
enough objects and we will need to move to less restricted RHMI.

Anyway concern is if number of buckets will be too big. And there comes question how does 0 correlation of bits affect number
of possible buckets.

6. Now it's time to work out an outline of thesis.

1. Introduction to similarity searching, motivation for sketch approach, etc.
2. Theoretical definitions and basic concepts of SimSearch, Sketch approach and Indexing etc. What's a good Sketch
3. Description of problem this Thesis focuses on (We got Sketches, how to Search among them? Why can't we just SeqScan? ...)
4. Currently existing approaches to this problem - MIH, maybe more?? Why it appears to be good approach for sketch searching?
5. Description of selected/designed approach. Some Index utilizing MIH, some baseline approach for comparison.
6. Description of experiments for evaluating this approach, questions we need to ask...
7. Description of implementation of experiments, dataset, etc...
8. Evaluation of experiments results - some graphs, results, conclusions and recommendations for parameter settings.
9. Conclusion, future work.

7.
Evaluation of idea of fixing r.

Number of possible candidates = m*(2^(m/p))
Number of buckets in index =

6.
Ok so I need to implement sketch transforming as well. This shouldn't be difficult because MIH is only appliable
for very shor sketches (8-16 maybe 32???). That might mean that correlations could be computed by brute force.

p' is how many bits we will generate. From this count we'll choose subset of p bits which results in least overall
correlation of bits between themselves. Then we'll save these sketches of uncorrelated bits and use them for indexing.

Sketch transformation will be realised via hyperplane partitioning.


Dataset is obtained from:
http://disa.fi.muni.cz/~xnovak8/neuralnet/

7. Sketch transformer
- should try to remove all possible tuples of bits to ensure small correlation
- we will use heuristic approach
- lets specify Obj. as MIN(SUM(corr(A,B)), where A,B are bits \in S, A!=B - single function can compute this on any
  sketch matrix
- now one by one, try to remove each bit, compute Obj.f. and in actual matrix remove bit with min. Obj. f. value

Complexity:
- Obj. computation requires for each couple of bits compute correlations of sketch matrix columns. Correlation computation
  for these columns can be approximated by random sampling.

  Complexity will be O(p^2*d), where sketch matrix shape is (p x d) assuming correlation computation is O(d). d is dataset
  length and p is sketch length.

  This will be repeated only p'-p times, where p' is longer sketch and p is sketch of desired length.

  Compexity of whole heuristics will be O((p'-p)*(p'^2*d)). - polynomial. With sketch len 32 and original sketch len 64
  on 100K dataset this will result in 32768*100000, which is too much....

We'll change Obj. to minimize mean of pairwise correlations between sketch matrix columns

Obj.: MIN SUM(MEAN([corr(A,B) for all (A,B) in sketch_matrix_columns if A!=B, idx_A<idx_B]

8.
Sketch transformer will generate a boolean mask for list of pivot couples. This mask will disable all undesirable pivot couples
when computing partitions. (Done it in different way after all, just removing columns of sketch matrix and then removing pivot couple)

9.
When computing 50/50 ratio |0.5-r| is used where r is balance ratio between 1 and 0 for this bit. The lower this number
the better balanced dataset is according to partitioning by this pivot couple (bit).

10.
SketchProducer is working well on 100K dataset, it's time to evaluate some results on 1M dataset - how much does it take to
generate sketches, whats the correlation, balance deviation for various bit lengths on this set. (Try 8, 16, 32)

11.
In this thesis the good index is index that responds to a query with a correct result and at the same time it minimizes the
number of computation required to answer the query. We will use number of dist comps as a search time metric.
Sketches are some encoding of multi-hyperplane partitioning. It's transformation which preserves if d(o1,o2)<d(o2,o3) => H(s1,s2)<H(s2,s3)
where d is disntance in original space and ds distance in sketch space.

=>
Search index returns all TP and some TF. We expect index to always return all TP among candates. If that holds our goal is to minimize
size of candidates which will result in minimizing count of TF.

Metrics:
Number of computation on sketches
Number of candidates provided

12. Multi Index hashing impl - r<m must hold, then only all sketches with at least 1 matching substrings need to be examined.

13. MIH stats:
1. mean bucket items count for various m
2. range query candidates count for various r and m
3. range query result count for various r and m
4. insert complexity (don't forget that)
5. hist bucket items count

So far experimets tels us, that lowering m is better but it affects range we can obtain objects up to. => this technique
is good up to some range on specific data, otherwise it doesn't make sense to use it.

- split data to 99000/1000
- evaluate on 1000
- implement persistence for stats counter
- add union count for range query to stats
- make stats counters for various r
- persist them somehow
- worry about visualization later

8b sketch
0 0 | 0 0 | 1 1 | 1 0
_ _
m=4
r<=3
=> aspon 1 exact match

13.
I should compare it to normal hash index

Next experiments:
a) what's the difference between expected upperbound and actual cardinality of rq candidate set?
b) how many buckets did search actually visited (some might not be existing)
c) when we instead of m use mean number of visited buckets, will difference in a) lower?
knn
- we stack multiple indexes and let them search until it finds k candidate set