{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\n",
    "    'main_folder':'/home/matej/data/sketch-testing/',\n",
    "    'save_allowed':True,\n",
    "    'dataset_file':'dataset.npy',\n",
    "    'testset_file': 'testset.npy',\n",
    "    'dev_mode': False,\n",
    "    'dev_mode_dataset_count_limit': 10000,\n",
    "    'dist_estimation_ref_obj_count': 100,\n",
    "    'dist_estimation_p_vals':[1,2],\n",
    "    'evaluation_set_query_obj_cnt':1000,\n",
    "    'evaluation_set_result_set_size':100,\n",
    "    'ghp_pivot_couple_candidates': 5000,\n",
    "    'min_balancing_score': 0.1,\n",
    "    'after_partitiong_checkpoint_file': 'after_partitioning.checkpoint'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d7ab1a9bf8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhamming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from scipy.spatial.distance import minkowski, hamming\n",
    "from fractions import Fraction \n",
    "from time import time\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy as sp\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel('INFO')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pfx(obj, msg, *args):\n",
    "    logger.info(obj.__class__.__name__+'| '+msg, *args)\n",
    "    \n",
    "def save_to(obj, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        cPickle.dump(obj,f)\n",
    "\n",
    "def load_from(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return cPickle.load(f)\n",
    "\n",
    "def show_text(title,xlabel,ylabel):\n",
    "    plt.suptitle(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "def heatmap(x,y,bins=50,figsize=(5,5),title=None, xlabel=None, ylabel=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    heatmap, xedges, yedges = np.histogram2d(x, y, bins)\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    plt.clf()\n",
    "    plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "    show_text(title,xlabel,ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def scatter(x,y,title=None, xlabel=None, ylabel=None, point_size=5):\n",
    "    plt.scatter(x, y, s=[point_size for _ in range(len(x))])\n",
    "    plt.suptitle(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def scatter_array(arr, title=None, xlabel=None, ylabel=None, point_size=5):\n",
    "    ser=pd.Series(arr)\n",
    "    plt.scatter(ser.index, ser, s=[point_size for _ in range(len(arr))])\n",
    "    plt.suptitle(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_array(arr, title=None, xlabel=None, ylabel=None,xticks=None):\n",
    "    pd.Series(arr).plot(grid=True)\n",
    "    plt.suptitle(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xticks:\n",
    "        plt.xticks(range(len(xticks)),xticks)\n",
    "    plt.show()\n",
    "\n",
    "def hist_array(arr,bins=50, title=None, xlabel=None, ylabel=None):\n",
    "    pd.Series(arr).hist(bins=bins,grid=True)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "def restrict_2d_space(x,y,x_interval, y_interval):\n",
    "    conditions=[x>=x_interval[0], x<=x_interval[1], y>=y_interval[0], y<=y_interval[1]]\n",
    "    mask=conditions[0]\n",
    "    for condition in conditions[1:]:\n",
    "        mask=np.logical_and(mask,condition)\n",
    "    return x[mask], y[mask]\n",
    "\n",
    "\n",
    "def balanced_correlation(col1, col2):\n",
    "    '''\n",
    "    Correlation when bits are balanced - |4*(a/n) - 1|, where a is CARD(NOT(XOR(A,B)))\n",
    "    '''\n",
    "    a=float(np.sum(np.logical_not(np.logical_xor(v1,v2))))\n",
    "    n=col1.shape[0]\n",
    "    return np.absolute((4.0*(a/n)) - 1.0)\n",
    "\n",
    "def percentage(part, whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "class IterationLogger(object):\n",
    "    def __init__(self, message='Started iteration no.: %s\\n Time from previous iteration: %ss\\n Time from start: %ss',log_by=1000):\n",
    "        self.log_by=log_by\n",
    "        self.num_iter=0\n",
    "        self.msg=message\n",
    "        self.time_started=None\n",
    "        self.time_this_iter=None\n",
    "        \n",
    "    def next_iter(self):\n",
    "        if self.time_this_iter is None:\n",
    "            self.time_this_iter=time()\n",
    "        if self.time_started is None:\n",
    "            self.time_started=time()\n",
    "        self.num_iter+=1\n",
    "        if self.num_iter%self.log_by==0:\n",
    "            dur_from_previous=time()-self.time_this_iter\n",
    "            self.time_this_iter=time()\n",
    "            dur_from_start=time()-self.time_started\n",
    "            log_pfx(self, self.msg, str(self.num_iter), str(dur_from_previous), str(dur_from_start))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=cfg['main_folder']+cfg['dataset_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(dataset_path, 'rb') as f:\n",
    "    decaf_vecs=np.load(f)\n",
    "if cfg['dev_mode']:\n",
    "    decaf_vecs=decaf_vecs[:cfg['dev_mode_dataset_count_limit']]\n",
    "obj_ids=np.arange(len(decaf_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decaf_vecs.shape #(vectors count, vector length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data (not included in database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_path=cfg['main_folder']+cfg['testset_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(testset_path, 'rb') as f:\n",
    "    test_vecs=np.load(f)\n",
    "test_ids=np.arange(len(decaf_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Distance distribution estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_minkowski_dist_distr(vec_matrix, ref_obj_cnt, p):\n",
    "    '''\n",
    "    p can be either single value or list of p parameters for minkowski distance\n",
    "    '''\n",
    "    distances=[list() for _ in range(len(p))]\n",
    "    reference_objs=vec_matrix[np.random.choice(vec_matrix.shape[0], size=ref_obj_cnt, replace=False)] #sampling without replacement\n",
    "    iter_log=IterationLogger(log_by=5000)\n",
    "    for decaf_vec in decaf_vecs:\n",
    "        iter_log.next_iter()\n",
    "        for ref_obj in reference_objs:\n",
    "            for i, val_p in enumerate(p):\n",
    "                dist=minkowski(ref_obj, decaf_vec, p=val_p)\n",
    "                distances[i].append(dist)\n",
    "    return tuple(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_l1, dist_l2 = estimate_minkowski_dist_distr(decaf_vecs,  cfg['dist_estimation_ref_obj_count'], cfg['dist_estimation_p_vals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_array(dist_l1, bins=50, title='L1 estimated distance distribution',ylabel='count object couples', xlabel='L1 distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_array(dist_l2, bins=50,title='L2 estimated distance distribution',ylabel='count object couples', xlabel='L2 distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GENERATE APPROXIMATION  EVALUATION SET\n",
    "We select 100 query objects and their 100 closest object's id's from the dataset to evaluate. It simulates the scenario where almost never more than 100 objects are required for result set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries_indicies=np.random.choice(decaf_vecs.shape[0], size=cfg['evaluation_set_query_obj_cnt'], replace=False)\n",
    "eval_queries_vecs=decaf_vecs[eval_queries_indicies] \n",
    "eval_queries_ids=obj_ids[eval_queries_indicies]\n",
    "eval_k=cfg['evaluation_set_result_set_size'] #k for knn to evaluation objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_objs=[list() for _ in range(len(eval_queries_ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds k-NN query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_log=IterationLogger(log_by=5000)\n",
    "for i in range(len(decaf_vecs)):\n",
    "    iter_log.next_iter()\n",
    "    candidate_vec=decaf_vecs[i]\n",
    "    candidate_id=obj_ids[i]\n",
    "    for j in range(len(eval_queries_vecs)):\n",
    "        query_vec=eval_queries_vecs[j]\n",
    "        dist=minkowski(candidate_vec, query_vec, p=1)\n",
    "        if len(eval_result_objs[j])>0:\n",
    "            if dist<eval_result_objs[j][-1][1]: #if object is not member of result set\n",
    "                eval_result_objs[j].append((candidate_id, dist))\n",
    "                eval_result_objs[j].sort(key=lambda x: x[1],reverse=False)\n",
    "                if len(eval_result_objs[j])>eval_k:#if > k objects remove last one\n",
    "                    del eval_result_objs[j][-1] \n",
    "        else:\n",
    "            eval_result_objs[j].append((candidate_id, dist))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_array([obj[1] for obj in eval_result_objs[17]],bins=20,title='Sample distance distribution of objects from result to query object',xlabel='L1 distance',ylabel='count objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GENERALIZED HYPERPLANE PARTITIONING PIVOTS SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_cand_cpl_cnt=cfg['ghp_pivot_couple_candidates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select random pivot couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_indicies=np.random.choice(decaf_vecs.shape[0], size=pivot_cand_cpl_cnt*2, replace=False)\n",
    "pivot_cand_vecs=decaf_vecs[cand_indicies]\n",
    "pivot_cand_ids=obj_ids[cand_indicies]\n",
    "pivot_cand_vecs_a, pivot_cand_vecs_b=np.split(pivot_cand_vecs,2)\n",
    "pivot_cand_ids_a, pivot_cand_ids_b=np.split(pivot_cand_ids,2)\n",
    "pivot_cand_vecs=np.array(zip(pivot_cand_vecs_a, pivot_cand_vecs_b))\n",
    "pivot_cand_ids=np.array(zip(pivot_cand_ids_a, pivot_cand_ids_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ghp_partition(vec, pivot_a, pivot_b, p=1):\n",
    "    '''\n",
    "    returns partition 0/1 and distance to closest pivot\n",
    "    '''\n",
    "    dist_a=minkowski(vec, pivot_a[0], p=p)\n",
    "    dist_b=minkowski(vec, pivot_b[1], p=p)\n",
    "    if dist_a<dist_b:\n",
    "        return 1, dist_a\n",
    "    if dist_a>dist_b:\n",
    "        return 0, dist_b\n",
    "    return np.random.choice([0,1]), dist_a\n",
    "    \n",
    "def compute_part_balance(cnt_partition, cnt_all):\n",
    "    '''\n",
    "    1 - 2*|0.5-x/y| where x is count members in left or right partition and y is count all members\n",
    "    '''\n",
    "    return 1.0-(2*abs(0.5-(cnt_partition/cnt_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot balance function\n",
    "plot_array([compute_part_balance(float(num),float(1000)) for num in range(0,1001)], title='GHP bit balance score',xlabel='x| no. objects in left partition',ylabel='b(x,1000)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition all objects from database and collect useful statistics for pivot couple selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions=np.zeros(shape=(decaf_vecs.shape[0], len(pivot_cand_ids)), dtype=np.float32)\n",
    "left_members_cnt=np.zeros(len(pivot_cand_ids), dtype=np.float32)\n",
    "sum_dist_from_ghp=np.zeros(len(pivot_cand_ids), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_log=IterationLogger(log_by=1000)\n",
    "for i, vec in enumerate(decaf_vecs):\n",
    "    iter_log.next_iter()\n",
    "    for j, pivot_cpl_vec in enumerate(pivot_cand_vecs):\n",
    "        partition,dist=get_ghp_partition(vec, pivot_cpl_vec[0], pivot_cpl_vec[1], p=1)\n",
    "        partitions[i][j]=partition\n",
    "        left_members_cnt[j]+=partition\n",
    "        sum_dist_from_ghp[j]+=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partitioning balance score\n",
    "balance_scores=np.array([compute_part_balance(float(cnt), float(decaf_vecs.shape[0])) for cnt in left_members_cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Partitioning with low balancing score is useless for filtering thus we'll remove such pivot couples right away. \n",
    "'''\n",
    "balance_limit=float(cfg['min_balancing_score'])\n",
    "bal_mask=np.array([i for i, score in enumerate(balance_scores) if score>=balance_limit])\n",
    "balance_scores=balance_scores[bal_mask]\n",
    "pivot_cand_vecs=pivot_cand_vecs[bal_mask]\n",
    "partitions=np.array(partitions).T[bal_mask].T\n",
    "left_members_cnt=np.array(left_members_cnt)[bal_mask]\n",
    "sum_dist_from_ghp=np.array(sum_dist_from_ghp)[bal_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute distance from dividing hyperplane score.\n",
    "Scale all distances to 0-1, 1 being the largest.\n",
    "HP distance score: (d_i - min(D))/(max(D)-min(D))\n",
    "'''\n",
    "avg_dist_from_hp=np.array([float(num)/decaf_vecs.shape[0] for num in sum_dist_from_ghp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_array(avg_dist_from_hp, bins=100,xlabel='Mean distance of objects from dividing hyperplane', \\\n",
    "           ylabel='count pivot couples',title='Distance of object from HP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We threw away all bits with balancing score below 0.1, from 5000 over 4000 reamained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array(sorted(balance_scores,reverse=True),title='Balance score of pivot couples', ylabel='balance score (1.0 - perfect 0.0 - worst)', xlabel='pivot couples sorted by balance score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(sorted(avg_dist_from_hp,reverse=True),title='Hyperplane distance of pivot couples', ylabel='HP distance score (1.0 - best 0.0 - worst)', xlabel='pivot couples sorted by HP distance score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['save_allowed']:\n",
    "    save_to((decaf_vecs, eval_queries_vecs,eval_queries_ids, eval_k, pivot_cand_vecs,eval_result_objs,\\\n",
    "             pivot_cand_ids, partitions, balance_scores, avg_dist_from_hp),\\\n",
    "            cfg['main_folder']+cfg['after_partitiong_checkpoint_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decaf_vecs, eval_queries_vecs,\\\n",
    "eval_queries_ids, eval_k, \\\n",
    "pivot_cand_vecs,eval_result_objs,\\\n",
    "pivot_cand_ids, partitions, balance_scores,\\\n",
    "avg_dist_from_hp=load_from(cfg['main_folder']+cfg['after_partitiong_checkpoint_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.argmax(balance_scores, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select balanced bits\n",
    "Without balance filtering would be useless. Not even we risk visiting parition with too many objects inside, probability of visiting such partition is greater than visiting the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_indicies=balance_scores.argsort()[::-1]\n",
    "balance_indicies=balance_indicies[:500]\n",
    "pivot_cand_vecs_bal=pivot_cand_vecs[balance_indicies]\n",
    "pivot_cand_ids_bal=pivot_cand_ids[balance_indicies]\n",
    "balance_scores_bal=balance_scores[balance_indicies]\n",
    "avg_dist_from_hp_bal=avg_dist_from_hp[balance_indicies]\n",
    "partitions_bal=partitions.T[balance_indicies].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array(sorted(balance_scores_bal,reverse=True),title='Balance score of pivot couples', ylabel='balance score (1.0 - perfect 0.0 - worst)', xlabel='pivot couples sorted by balance score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE CORRELATED BITS\n",
    "In this section we evaluated three heuristics for uncorrelated bit subset selection. Two of them are optimized either for maximal correlation or mean correlation. Maximal correlation limits the worst bit rendundancy and Mean correlation represents overall information redundancy in sketchces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr_mtx(sketch_matrix):\n",
    "    corr_mtx = np.absolute(np.corrcoef(sketch_matrix.T))\n",
    "    np.fill_diagonal(corr_mtx, 0.0)\n",
    "    return corr_mtx\n",
    "\n",
    "def compute_mean_corr(corr_mtx, include=None, exclude=None):\n",
    "    if include is None and exclude is None:\n",
    "        u=np.triu_indices(n=corr_mtx.shape[0], m=corr_mtx.shape[1], k=1)\n",
    "        return np.mean(corr_mtx[u])\n",
    "    if include is not None:\n",
    "        selection=corr_mtx[include].T[include]\n",
    "        u=np.triu_indices(n=selection.shape[0], m=selection.shape[1], k=1)\n",
    "        return np.mean(selection[u])\n",
    "    if exclude is not None:\n",
    "        mask = np.ones(corr_mtx.shape[0],dtype=bool)\n",
    "        mask[exclude]=0\n",
    "        selection=corr_mtx[mask].T[mask]\n",
    "        u=np.triu_indices(n=selection.shape[0], m=selection.shape[1], k=1)\n",
    "        return np.mean(selection[u])\n",
    "\n",
    "def compute_max_corr(corr_mtx, include=None, exclude=None):\n",
    "    if include is None and exclude is None:\n",
    "        u=np.triu_indices(n=corr_mtx.shape[0], m=corr_mtx.shape[1], k=1)\n",
    "        return np.max(corr_mtx[u])\n",
    "    if include is not None:\n",
    "        selection=corr_mtx[include].T[include]\n",
    "        u=np.triu_indices(n=selection.shape[0], m=selection.shape[1], k=1)\n",
    "        return np.max(selection[u])\n",
    "    if exclude is not None:\n",
    "        mask = np.ones(corr_mtx.shape[0],dtype=bool)\n",
    "        mask[exclude]=0\n",
    "        selection=corr_mtx[mask].T[mask]\n",
    "        u=np.triu_indices(n=selection.shape[0], m=selection.shape[1], k=1)\n",
    "        return np.max(selection[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=compute_corr_mtx(partitions_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_lengths=range(0,128,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1\n",
    "Select K random subsets of specified length and choose the one with minimal pairwise correlation. This is a baseline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_bit_subset(sketch_matrix, desired_length, k, by='max'):\n",
    "    if desired_length<2:\n",
    "        return 0.0,0.0,0.0\n",
    "    indices=np.array(range(sketch_matrix.shape[1]))\n",
    "    subsets=np.array([np.random.choice(indices,size=desired_length, replace=False) for _ in range(k)])\n",
    "    best_subset=None\n",
    "    best_corr_max=1.0\n",
    "    best_corr_mean=1.0\n",
    "    for subset_idx in subsets:\n",
    "        if by=='max':\n",
    "            corr_max=compute_max_corr(M, include=subset_idx)\n",
    "            if corr_max<=best_corr_max:\n",
    "                best_corr_max=corr_max\n",
    "                best_subset=subset_idx\n",
    "                best_corr_mean=compute_mean_corr(M, include=subset_idx)\n",
    "        if by=='mean':\n",
    "            corr_mean=compute_mean_corr(M, include=subset_idx)\n",
    "            if corr_mean<=best_corr_mean:\n",
    "                best_corr_mean=corr_mean\n",
    "                best_subset=subset_idx\n",
    "                best_corr_max=compute_max_corr(M, include=subset_idx)\n",
    "    return best_subset, best_corr_max, best_corr_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1000\n",
    "found_corrs_max=[list(),list()] #first by max, second by mean\n",
    "found_corrs_mean=[list(),list()]\n",
    "for sketch_length in sketch_lengths:\n",
    "    _, best_corr_max, best_mean_corr=select_random_bit_subset(partitions_bal, sketch_length,k=K,by='max')\n",
    "    found_corrs_max[0].append(best_corr_max)\n",
    "    found_corrs_mean[0].append(best_mean_corr)\n",
    "    _, best_corr_max, best_mean_corr=select_random_bit_subset(partitions_bal, sketch_length,k=K,by='mean')\n",
    "    found_corrs_max[1].append(best_corr_max)\n",
    "    found_corrs_mean[1].append(best_mean_corr)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 1 - optimized for max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array(found_corrs_max[0],title='Max pair-wise correlation of randomly selected subsets of bits', ylabel='Max pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_array(found_corrs_mean[0],title='Mean pair-wise correlation of randomly selected subsets of bits', ylabel='Mean pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 1 - optimized for mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_max[1],title='Max pair-wise correlation of randomly selected subsets of bits', ylabel='Max pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_mean[1],title='Mean pair-wise correlation of randomly selected subsets of bits', ylabel='Mean pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 2\n",
    "Select bits with minimal sum of values in correlation matrix M. This algorithm is O(n) and provides quite good results especially in terms of mean bit correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_corrs_max=list()\n",
    "found_corrs_mean=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for length in sketch_lengths:\n",
    "    if length<2:\n",
    "        found_corrs_max.append(0.0)\n",
    "        found_corrs_mean.append(0.0)\n",
    "    else:\n",
    "        indices=np.sum(M,axis=1).argsort()[:length]\n",
    "        found_corrs_max.append(compute_max_corr(M, include=indices))\n",
    "        found_corrs_mean.append(compute_mean_corr(M, include=indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_max, title='Max pair-wise correlation', ylabel='Max pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that even though maximum is high (0.5) further selected bits do not increase mean pair-wise correlation much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_mean,title='Mean pair-wise correlation', ylabel='Mean pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3\n",
    "Start with initial selected least correlated column (smallest sum in row) and add one in each iteration, which will result to minimal increase in correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_uncorrelated_bits(corr_mtx,desired_length,by='max'):\n",
    "    '''\n",
    "    Returned list of indices is sorted so least correlated sketches of lower size can be selected by indices[:desired_size].\n",
    "    '''\n",
    "    if desired_length<2:\n",
    "        return 0.0,0.0,0.0\n",
    "    least_correlated=np.argmin(np.sum(corr_mtx, axis=1))\n",
    "    indices=list() #to keep them sorted\n",
    "    indices_lookup=set() #to speed up search for membership\n",
    "    indices.append(least_correlated)\n",
    "    indices_lookup.add(least_correlated)\n",
    "    corr_max=None\n",
    "    corr_mean=None\n",
    "    while(len(indices)<desired_length):\n",
    "        best_i=None\n",
    "        best_max_corr=100\n",
    "        best_mean_corr=100\n",
    "        for i in range(corr_mtx.shape[0]):                    \n",
    "            if i not in indices_lookup:\n",
    "                if by=='max':\n",
    "                    corr_max=compute_max_corr(corr_mtx, include=list(indices)+[i])\n",
    "                    if corr_max<best_max_corr:\n",
    "                        best_i=i\n",
    "                        best_max_corr=corr_max\n",
    "                        best_mean_corr=compute_mean_corr(corr_mtx, include=list(indices)+[i])\n",
    "                if by=='mean':\n",
    "                    corr_mean=compute_mean_corr(corr_mtx, include=list(indices)+[i])\n",
    "                    if corr_mean<best_mean_corr:\n",
    "                        best_i=i\n",
    "                        best_mean_corr=corr_mean\n",
    "                        best_max_corr=compute_max_corr(corr_mtx, include=list(indices)+[i])\n",
    "        indices_lookup.add(best_i)\n",
    "        indices.append(best_i)\n",
    "        corr_max=best_max_corr\n",
    "        corr_mean=best_mean_corr\n",
    "    return indices, corr_max, corr_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_corrs_max=[list(),list()] #first by max, second by mean\n",
    "found_corrs_mean=[list(),list()]\n",
    "for sketch_len in sketch_lengths:\n",
    "    _,corr_max, corr_mean=select_uncorrelated_bits(M, sketch_len,by='max')\n",
    "    found_corrs_max[0].append(corr_max)\n",
    "    found_corrs_mean[0].append(corr_mean)\n",
    "    _,corr_max, corr_mean=select_uncorrelated_bits(M, sketch_len,by='mean')\n",
    "    found_corrs_max[1].append(corr_max)\n",
    "    found_corrs_mean[1].append(corr_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 3 - optimized by max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_max[0], title='Max pair-wise correlation', ylabel='Max pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_mean[0],title='Mean pair-wise correlation', ylabel='Mean pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 3 - optimized by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_max[1], title='Max pair-wise correlation', ylabel='Max pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that you can get some bit with correlation almost 0.5 but still overall information redundancy among bits is below or close to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(found_corrs_mean[1],title='Mean pair-wise correlation', ylabel='Mean pair-wise Pearson Correlation', xlabel='Sketch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING SKETCH DATABASE FOR EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already transformed whole database into sketches of length over 4000 and then chosen 500 bits with top balance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_bal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll filter out unbalanced and correlated bits using Algorithm 3 with scoring by mean pairwise correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, corr_max, corr_mean=select_uncorrelated_bits(M,128,by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_max #worst pair-wise correlation in our 128 bit sketchces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_mean #mean pair-wise correlation in our 128 bit sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform whole database to uncorrelated sketchces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_db=partitions_bal.T[indices].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rembember selected pivot couples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghp_pivot_vecs=pivot_cand_vecs_bal[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ghp_pivot_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of class transforming L_{p} vectors to sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinkowskiSketchCreator(object):\n",
    "    def __init__(self, pivot_cpl_vecs, p=1,limit_len=None):\n",
    "        self.pivots=pivot_cpl_vecs\n",
    "        if limit_len is not None:\n",
    "            self.pivots=self.pivots[:limit_len]\n",
    "        self.p=p\n",
    "        \n",
    "    def get_ghp_partition(self, vec, pivot_a, pivot_b):\n",
    "        '''\n",
    "        returns partition 0/1 and distance to closest pivot\n",
    "        '''\n",
    "        dist_a=minkowski(vec, pivot_a, p=self.p)\n",
    "        dist_b=minkowski(vec, pivot_b, p=self.p)\n",
    "        if dist_a<dist_b:\n",
    "            return 1\n",
    "        if dist_a>dist_b:\n",
    "            return 0\n",
    "        return np.random.choice([0,1])\n",
    "    \n",
    "    def get_sketch(self, vec, limit_len=None):\n",
    "        if limit_len is None:\n",
    "            return np.array([self.get_ghp_partition(vec, pivot_cpl[0],pivot_cpl[1])for pivot_cpl in self.pivots], dtype=np.float32)\n",
    "        else:\n",
    "            return np.array([self.get_ghp_partition(vec, pivot_cpl[0],pivot_cpl[1])for pivot_cpl in self.pivots[:limit_len]], dtype=np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_creator=MinkowskiSketchCreator(ghp_pivot_vecs,limit_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform test set to sketchces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sketch_db=np.array([sketch_creator.get_sketch(vec) for vec in test_vecs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketch properties Train vs Test dataset\n",
    "We created sketches of test dataset which also contains decaf vectors but not those from training (sketch creation) dataset. No we'll examine whether and how did Sketch properties change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sketch_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sketch_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_score(db, aggregation_f=np.mean):\n",
    "    return aggregation_f(np.array([compute_part_balance(sum(col),col.shape[0]) for col in db.T]))\n",
    "\n",
    "def correlation(db, by='max'):\n",
    "    corr_mtx=compute_corr_mtx(db)\n",
    "    if by=='max':\n",
    "        return compute_max_corr(corr_mtx)\n",
    "    if by=='mean':\n",
    "        return compute_mean_corr(corr_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balance\n",
    "Balancing score of both is almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balance_score(sketch_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_score(test_sketch_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Train vs Test\n",
    "Correlation both max and mean turned out to be almost the same on both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation(sketch_db, by='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(test_sketch_db, by='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(sketch_db, by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(test_sketch_db, by='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPROXIMATION QUALITY EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_dist(a,b):\n",
    "    return np.count_nonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_lengths=[2,4,8,16,32,64,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_results=dict()\n",
    "for sketch_length in sketch_lengths:\n",
    "    queries=np.array([sketch_creator.get_sketch(vec, limit_len=sketch_length) for vec in eval_queries_vecs])\n",
    "    database=sketch_db.T[:sketch_length].T\n",
    "    query_true_results=np.array([np.array([tupl[0] for tupl in res_list]) for res_list in eval_result_objs])\n",
    "    distances=np.zeros(shape=(queries.shape[0],database.shape[0]))\n",
    "    for i, query in enumerate(queries):\n",
    "        for j, sketch in enumerate(database):\n",
    "            distances[i][j]=hamming_dist(query,sketch)\n",
    "    result_ids=np.array([np.argsort(dist) for dist in distances])\n",
    "\n",
    "    filtering_counts=range(0, 67000, 5000)\n",
    "    percentages=[100-int(percentage(cnt, result_ids[0].shape[0])) for cnt in filtering_counts]\n",
    "\n",
    "    mean_intersection=list()\n",
    "    for count in filtering_counts:\n",
    "        mean_intersection.append(np.mean(np.array([len(set(query_true_results[i])&set(result_ids[i][:count])) for i in range(query_true_results.shape[0])])))\n",
    "    approx_results[sketch_length]=mean_intersection\n",
    "    print(approx_results[sketch_length])\n",
    "    plot_array(mean_intersection,xticks=percentages, title='Mean % query result covered in pre-selected (sketch length: '+str(sketch_length)+')', ylabel='Mean % correct in pre-selected', xlabel='% of db filtered ')\n",
    "    #plot_array(mean_intersection, title='Mean % query result covered in pre-selected (sketch length: '+str(sketch_length)+')', ylabel='Mean % correct in pre-selected', xlabel='% of db filtered ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# approx_results=dict()\n",
    "# for sketch_length in sketch_lengths:\n",
    "#     queries=np.array([sketch_creator.get_sketch(vec, limit_len=sketch_length) for vec in eval_queries_vecs])\n",
    "#     database=sketch_db.T[:sketch_length].T\n",
    "#     query_true_results=np.array([np.array([tupl[0] for tupl in res_list]) for res_list in eval_result_objs])\n",
    "#     distances=np.zeros(shape=(queries.shape[0],database.shape[0]))\n",
    "#     for i, query in enumerate(queries):\n",
    "#         for j, sketch in enumerate(database):\n",
    "#             distances[i][j]=hamming_dist(query,sketch)\n",
    "#     result_ids=np.array([np.argsort(dist) for dist in distances])\n",
    "\n",
    "#     filtering_counts=range(0, 67000, 5000)\n",
    "#     percentages=[int(percentage(cnt, result_ids[0].shape[0])) for cnt in filtering_counts]\n",
    "\n",
    "#     mean_intersection=list()\n",
    "#     for count in filtering_counts:\n",
    "#         mean_intersection.append(np.mean(np.array([len(set(query_true_results[i])&set(result_ids[i][:count])) for i in range(query_true_results.shape[0])])))\n",
    "#     approx_results[sketch_length]=mean_intersection\n",
    "#     print(approx_results[sketch_length])\n",
    "#     plot_array(mean_intersection,xticks=percentages, title='Mean % query result covered in pre-selected (sketch length: '+str(sketch_length)+')', ylabel='Mean % correct in pre-selected', xlabel='% DB searched ')\n",
    "#     #plot_array(mean_intersection, title='Mean % query result covered in pre-selected (sketch length: '+str(sketch_length)+')', ylabel='Mean % correct in pre-selected', xlabel='% of db filtered ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
